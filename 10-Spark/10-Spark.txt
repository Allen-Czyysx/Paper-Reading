// https://www.kancloud.cn/kancloud/spark-internals/45242

问题: MapReduce系统无法再利用输出结果, 对需要迭代的ML算法和交互式数据挖掘应用不友好. 故提出Spark, 在解决这些问题的同时, 维持可扩展和容错.

RDD: 分布在集群上的只读数据块.
	1) RDD默认懒惰 (只在需要的时候被生成), 短暂 (用完即抛弃), 但用户可指定RDD保留在内存或磁盘中供之后的计算使用.
	2) 对每个RDD维护一个lineage信息, 包含它的parent和如何对parent做转换. 使得RDD的partition丢失时有足够的信息重建它.

------------------------

问题: 对要reuse计算结果 (迭代的ML算法, 交互式数据挖掘) 的场景, 把数据放在内存中更加高效 (Hadoop等传递的是磁盘中的数据, 缺乏对分布式内存的利用). 故提出RDD, 同时提供容错.

贡献:
	1) RDD: 已有的系统要么将计算结果放在磁盘中, 要么不支持更广泛的reuse方式. RDD则充分利用了分布式内存, 同时也支持更广泛的应用场景.
	2) 容错效率: 已有实现 (DSM) 是基于细粒度的状态更新 (如一个cell的修改), 需要在集群中记日志或备份, 不适合数据密集型应用. RDD的容错基于粗粒度的转换 (如对很多数据的相同修改操作), 从而只需记录转换操作 (lineage graph) 的日志.

RDD:
	1) 与DSM相比: 以不支持细粒度写来换取更高效的容错, 且恢复时不需要回滚整个程序; 只读让backup plan实现简单, 而DSM会因为写同一内容而发生冲突; scheduler可利用数据位置来优化任务派发 (脑补: 而DSM的数据位置不可见).
	2) interface: partitions(), preferredLocations(p) (通过该接口可尽量将任务调度给拥有数据的节点), dependencies(), iterator(p, parentIters), partitioner(). 不同类型的RDD实现不同.

实现:
	1) scheduler根据RDD interface获得的信息分派任务 (//TODO 怎么实现locality? 完成一个算子再选择下一个worker? 还是可直接推算出RDD的存储位置?).
	2) 内存管理: RDD的partition的换出基本按照LRU策略, 但会考虑相同RDD的不同partition的使用情况 (往往一个partition的使用意味着将来也会使用其他partition).
	3) checkpoint: 由于RDD只读, 所以实现简单 (复制时无需暂停程序或分布式快照方案).

------------------------

问题: 流处理系统中的faults, stragglers的快速处理. 当时的容错机制要么基于副本 (2x硬件资源), 要么基于上游备份 (线性恢复, 慢), 且两者都没有处理stragglers.

大规模系统的两个主要问题: 
	1) faults.
	2) stragglers.

实现:
	1) 采用微批处理, 利用批处理的fault & straggler tolerent机制, 解决流处理系统的痛点.
	2) 尽量降低微批处理带来的延迟, 使用RDDs作为job间传递的单位 (而不是磁盘中的数据).
	3) 流处理中每个算子是long-lived且stateful (例如维护一个count表) 的, 其中算子状态的副本同步很麻烦. 微批处理把计算状态存在RDDs中, 使得计算无状态且确定.
	4) 确定性允许RDDs的恢复依赖于lineage graph (checkpoint间隔30s下1-2s恢复).

对无序的输入要么等待一段时间, 要么用户自己在代码中处理 (如在代码中检查晚到的数据, 处理这些并更新之前batch的结果).
