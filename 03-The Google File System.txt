目标: 分布式文件系统, 提供容错机制, 部署在廉价机器上.
	1) 把出错当做常态 (监视, 检错, 容错, 自动恢复).
	2) 数据量大 (I/O操作, 存储块大小).
	3) 写操作大多是append而非overwrite (关注append的优化和原子性保障).
	4) 与上层应用协调设计 (适当放宽一致性, 又不给应用带来沉重负担).

假设:
	1) 部署的机器廉价.
	2) GB规模文件很常见 (同时也支持小文件, 但不对这些做优化).
	3) 负载的读由大型顺序读 (1MB或更多) 和小型随机读 (几KB) 组成.
	4) 负载的写由大型顺序写和小型随机写 (支持但不做优化) 组成.
	5) 需要保障append的原子性 (多个客户端同时append同一个文件).
	6) 高带宽比低延迟更重要.

结构:
	1) 一个master: 维护整个文件系统的元数据 (命名空间, file-chunk表, chunk地址), 负责system-wide活动.
	2) 多个chunk server: 存储数据.

实现:
	1) 读写: Master基本不参与 (与客户端间仅控制流, 无数据流; 客户端缓存元数据, 进一步减少master的负载), 避免成为瓶颈.
	2) chunk大小: 64MB (较大). 好处在于减少客户端请求数, 网络负载, 元数据量; 坏处在于对于小文件的访问会集中在一个chunk上.
	3) 元数据: 放主存, 提高速度. 其中chunk地址不持久化, 通过心跳维护; 命名空间, file-chunk表以log形式持久化, 并备份.
	4) 一致性: 元数据修改为单点; consistant但undefined的情况需要上层应用处理 (如checkpoint); 重复记录需要上层应用处理 (如checksum).
	5) 数据流: 链式 (pipelined) 传输到主备节点上, 充分利用网络带宽.
	6) at-least-once: 客户端会处理主副本的不一致 (如重试append, 会导致记录重复), 故GFS仅保证.
	7) 快照: 写时复制策略. 收回chunkserver租约, 新的写操作会促使master让chunkserver本地复制一份chunk.
	8) 数据完整性: checksum. 发现数据出错, chunkserver会向master反馈, master指挥副本节点复制正确的chunk.
